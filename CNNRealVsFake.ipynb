{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ory4PG6D47gz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSWYKnI6drWb"
      },
      "outputs": [],
      "source": [
        "# Import zip file\n",
        "def import_images(path):\n",
        "   realimgs = []\n",
        "   fakeimgs = []\n",
        "   with zipfile.ZipFile(path, 'r') as zip:\n",
        "       for file in zip.namelist():\n",
        "           if \"real\" in file.lower():\n",
        "                try:\n",
        "                   image_file = zip.open(file)\n",
        "                   image_data = image_file.read()\n",
        "                   image = Image.open(BytesIO(image_data))\n",
        "                   image = image.crop((0,0,image.size[0]-98, image.size[1]-20)).resize((500,500))\n",
        "                   realimgs.append(image)\n",
        "                except:\n",
        "                    pass\n",
        "           if \"fake\" in file.lower():\n",
        "               try:\n",
        "                   image_file = zip.open(file)\n",
        "                   image_data = image_file.read()\n",
        "                   image = Image.open(BytesIO(image_data))\n",
        "                   image = image.crop((0,0,image.size[0]-98, image.size[1]-20)).resize((500,500))\n",
        "                   fakeimgs.append(image)\n",
        "               except:\n",
        "                   pass\n",
        "   return (realimgs, fakeimgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run ONLY once!\n",
        "\n",
        "realimgs, fakeimgs = import_images(\"/Users/aditya/Desktop/Training Dataset.zip\")\n",
        "with open(\"realimgs\", \"wb\") as txt: \n",
        "    pickle.dump(realimgs, txt)\n",
        "with open(\"fakeimgs\", \"wb\") as txt: \n",
        "    pickle.dump(fakeimgs, txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJp4bAr-GIe0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the dataset class\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, real_dir, fake_dir, transform=None):\n",
        "        with open(\"realimgs\", \"wb\") as txt: \n",
        "            self.real_images = pickle.load(txt)\n",
        "        with open(\"fakelimgs\", \"wb\") as txt: \n",
        "            self.fake_images = pickle.load(txt)\n",
        "        self.all_images = self.real_images + self.fake_images\n",
        "        self.labels = [1] * len(self.real_images) + [0] * len(self.fake_images)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.all_images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oHcVQJY5KV7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 28 * 28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uFWHYnD15HIE"
      },
      "outputs": [],
      "source": [
        "# Set up data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset and split into train and validation sets\n",
        "dataset = ImageDataset('path/to/real/images', 'path/to/fake/images', transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"mps\")\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "7qJdrI2c5A4x",
        "outputId": "23a7d853-44ea-47e8-8d1a-d0a3cdd69df0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        print(\"Done with image\")\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "    torch.save(model.state_dict(), 'cnn_model' + str(epoch) + '.pth')\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'cnn_model.pth')\n",
        "\n",
        "print(\"Training complete. Model saved as 'cnn_model.pth'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnuWL2SBuwsq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
